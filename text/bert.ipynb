{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#整理文本为bert适合的格式\n",
    "def process_text_for_bert(record_path = \"//10.200.42.124/videos/segments\",\n",
    "                    ori_path = \"data/text\",\n",
    "                    save_path = \"data/bert/samples\"):\n",
    "    '''\n",
    "    将精彩片段抽取出来，正例反例分开\n",
    "    :param record_path:\n",
    "    :param ori_path:\n",
    "    :param save_path:\n",
    "    :return:\n",
    "    '''\n",
    "    #with open(\"data/stopwords.txt\",encoding='utf8') as f:\n",
    "        #stop_words=set(f.read().split('\\n'))\n",
    "    stop_words = ['囡', '嗯', '吖', '唉', '呀']\n",
    "\n",
    "    count=0\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    pf=open(os.path.join(save_path,'positive.txt'),'w',encoding='utf8')\n",
    "    nf=open(os.path.join(save_path,'negative.txt'),'w',encoding='utf8')\n",
    "    for date_path in os.listdir(ori_path):\n",
    "        for text_path in os.listdir(os.path.join(ori_path,date_path)):\n",
    "            print(\"processing... \",ori_path,date_path,text_path)\n",
    "            highlights = set()\n",
    "            # if exists highlights\n",
    "            if os.path.exists(os.path.join(record_path,date_path,text_path[:-4],'pos','p')):\n",
    "                good_frames=os.listdir(os.path.join(record_path,date_path,text_path[:-4],'pos','p'))\n",
    "                if not good_frames:\n",
    "                    print(\"no highlights\")\n",
    "                    continue\n",
    "                for frame in good_frames:\n",
    "                    try:\n",
    "                        minute=((int(frame.split('.')[0])-3)*10+9)//60\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(frame)\n",
    "                    else:\n",
    "                        if minute<0:\n",
    "                            minute=0\n",
    "                        highlights.add(minute)\n",
    "            # no highlights\n",
    "            else:\n",
    "                print(\"no highlights\")\n",
    "                continue\n",
    "            print(\"highlights: \",highlights)\n",
    "            count+=1\n",
    "            with open(os.path.join(ori_path,date_path,text_path),encoding='utf8') as f:\n",
    "                for i,line in enumerate(f):\n",
    "                    line=line.strip()\n",
    "                    if not line: # empty line (minute)\n",
    "                        continue\n",
    "                    #line=line.replace(' ','。')\n",
    "                    for sw in stop_words:\n",
    "                        line=line.replace(sw,'')\n",
    "                    \n",
    "                    if i in highlights:\n",
    "                        pf.write(line+'\\n')\n",
    "                    else:\n",
    "                        nf.write(line+'\\n')\n",
    "    pf.close()\n",
    "    nf.close()\n",
    "    print(\"number of valid files:\",count)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_text_for_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取正样本特征\n",
    "bert_path=\"C:\\\\Users\\\\v_sunhao7\\\\PycharmProjects\\\\bert\"\n",
    "bert_model_path=\"C:\\\\Users\\\\v_sunhao7\\\\PycharmProjects\\\\bert\\\\chinese_L-12_H-768_A-12\"\n",
    "output_path=\"data/bert/parameter/\"\n",
    "input_path=\"data/bert/samples/\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "cmd=\"python %s/extract_features.py \\\n",
    "  --input_file=%s \\\n",
    "  --output_file=%s \\\n",
    "  --vocab_file=%s/vocab.txt \\\n",
    "  --bert_config_file=%s/bert_config.json \\\n",
    "  --init_checkpoint=%s/bert_model.ckpt \\\n",
    "  --layers=-1 \\\n",
    "  --max_seq_length=512 \\\n",
    "  --batch_size=8\"%(bert_path, input_path+\"positive.txt\", output_path+\"positive.json\", bert_model_path, bert_model_path, bert_model_path)\n",
    "\n",
    "subprocess.call(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取负样本特征\n",
    "bert_path=\"C:\\\\Users\\\\v_sunhao7\\\\PycharmProjects\\\\bert\"\n",
    "bert_model_path=\"C:\\\\Users\\\\v_sunhao7\\\\PycharmProjects\\\\bert\\\\chinese_L-12_H-768_A-12\"\n",
    "output_path=\"data/bert/parameter/\"\n",
    "input_path=\"data/bert/samples/\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "cmd=\"python %s/extract_features.py \\\n",
    "  --input_file=%s \\\n",
    "  --output_file=%s \\\n",
    "  --vocab_file=%s/vocab.txt \\\n",
    "  --bert_config_file=%s/bert_config.json \\\n",
    "  --init_checkpoint=%s/bert_model.ckpt \\\n",
    "  --layers=-1 \\\n",
    "  --max_seq_length=512 \\\n",
    "  --batch_size=8\"%(bert_path, input_path+\"negative.txt\", output_path+\"negative.json\", bert_model_path, bert_model_path, bert_model_path)\n",
    "\n",
    "subprocess.call(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#整理成矩阵并保存\n",
    "\n",
    "pos_matrix=np.zeros((0,768))\n",
    "with open(\"data/bert/parameter/positive.json\", encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        t=json.loads(line)\n",
    "        pos_matrix=np.vstack((pos_matrix,np.array(t['features'][0]['layers'][0]['values'])))\n",
    "        \n",
    "neg_matrix=np.zeros((0,768))\n",
    "with open(\"data/bert/parameter/negative.json\", encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        t=json.loads(line)\n",
    "        neg_matrix=np.vstack((neg_matrix,np.array(t['features'][0]['layers'][0]['values'])))\n",
    "        \n",
    "np.save('data/bert/matrix/pos_matrix.npy',pos_matrix)\n",
    "np.save('data/bert/matrix/neg_matrix.npy',neg_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch做分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #self.fc1 = nn.Linear(768, 32)\n",
    "        self.fc1 = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "pos_matrix=np.load(\"data/bert/matrix/pos_matrix.npy\")\n",
    "neg_matrix=np.load(\"data/bert/matrix/neg_matrix.npy\")\n",
    "#shuffle\n",
    "np.random.shuffle(pos_matrix)\n",
    "np.random.shuffle(neg_matrix)\n",
    "#split data\n",
    "train_prop=0.7\n",
    "repeats=4  #训练集中，正例的复制倍数\n",
    "pos_train_size=int(pos_matrix.shape[0]*train_prop)\n",
    "neg_train_size=int(neg_matrix.shape[0]*train_prop)\n",
    "\n",
    "train_pos=pos_matrix[:pos_train_size]\n",
    "train_neg=neg_matrix[:neg_train_size]\n",
    "train_pos=np.tile(train_pos,(repeats,1))\n",
    "test_pos=pos_matrix[pos_train_size:]\n",
    "test_neg=neg_matrix[neg_train_size:]\n",
    "#test_pos=np.tile(test_pos,(6,1))\n",
    "\n",
    "x_train=np.vstack((train_pos,train_neg))\n",
    "y_train=np.vstack((np.ones((len(train_pos),1)),np.zeros((len(train_neg),1))))\n",
    "x_test=np.vstack((test_pos,test_neg))\n",
    "y_test=np.vstack((np.ones((len(test_pos),1)),np.zeros((len(test_neg),1))))\n",
    "\n",
    "#convert to pytorch tensor\n",
    "x_train=torch.from_numpy(x_train)\n",
    "x_test=torch.from_numpy(x_test)\n",
    "y_train=torch.from_numpy(y_train)\n",
    "y_test=torch.from_numpy(y_test)\n",
    "#data loader\n",
    "trainset=torch.utils.data.TensorDataset(x_train, y_train)\n",
    "testset=torch.utils.data.TensorDataset(x_test, y_test)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_neg)/len(test_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    print(\"training...... epoch:\", epoch)\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "    print(\"loss:\", running_loss/i)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train f score\n",
    "precision=torch.sum(torch.round(net(Variable(x_train[:].float())))*y_train.float())/torch.sum(torch.round(net(Variable(x_train[:].float()))))\n",
    "recall=torch.sum(torch.round(net(Variable(x_train[:].float())))*y_train.float())/torch.sum(y_train.float())\n",
    "f1=precision*recall*2/(precision+recall)\n",
    "print(precision,recall,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test f score\n",
    "precision=torch.sum(torch.round(net(Variable(x_test[:].float())))*y_test.float())/torch.sum(torch.round(net(Variable(x_test[:].float()))))\n",
    "recall=torch.sum(torch.round(net(Variable(x_test[:].float())))*y_test.float())/torch.sum(y_test.float())\n",
    "f1=precision*recall*2/(precision+recall)\n",
    "print(precision,recall,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net,\"model/bert.output.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
